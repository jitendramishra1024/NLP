# -*- coding: utf-8 -*-
"""CONSUMER_COMPLAINT_CLASSIFICATION_USINNG_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_WsfqZiMgvhBvv4M_dwwM9s6JXho59j
"""

from google.colab import drive
drive.mount('/content/drive')

#PROBLEM FORMULATION
#The problem is supervised text classification problem, and our goal is to investigate which supervised machine learning 
#methods are best suited to solve it.
#Given a new complaint comes in, we want to assign it to one of 12 categories.

#PART 01 :
#DATA EXPLORATION 
import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Consumer_Complaints.csv')
#remove all column except input and output 
col = ['Product', 'Consumer Complaint']
df= df[col]
#remove null review comments 
df= df[pd.notnull(df['Consumer Complaint'])]
#rename the column from Consumer Complaint to Consumer_complaint
df.columns=['Product', 'Consumer_complaint']
#factorize the product and rename with categoryid 
df['category_id'] = df['Product'].factorize()[0]
#remove duplicates 
#remove product id 
df =df.drop(df.loc[:,['Product']], axis =1 )
df=df.drop_duplicates()

df.head()

df['category_id'].value_counts()

#DATA CLEANING 
import re
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
lemmatizer = WordNetLemmatizer()
ps = PorterStemmer()

def clean_text(df):
    all_reviews = list()
    lines = df["Consumer_complaint"].values.tolist()
    for text in lines:
        text = text.lower()
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = text.replace('x', '')
        #remove single character 
        text = re.sub('\s\w\s',' ',text)
        words=nltk.word_tokenize(text)
        stop_words= set(stopwords.words('english'))
        stop_words.discard("not")
        words_without_stop_words=[word for word in words if word not in stop_words]
        #words=[lemmatizer.lemmatize(word) for word in words_without_stop_words ]
        words=[ps.stem(word) for word in words_without_stop_words ]
        words = ' '.join(words)
        all_reviews.append(words)
    return all_reviews

all_complaint = clean_text(df)
all_complaint[0:20]

#FIRST We will make all text data into numrical and of same length 
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 250
# This is fixed.
EMBEDDING_DIM = 100

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences 
#Word Embedding is a representation of text where words that have the same meaning have a similar representation.Embedding layers 
#hrlps to represnts text in vector format 
#pad sequences to make vector of equal length if vector is not that much length then 0s will be added 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM 
from tensorflow.keras.layers import Dense 
import gensim
import os
from tensorflow.python.keras.preprocessing.text import Tokenizer

#number of words in each review/ happymoments/sentence=55
validation_split = 0.2
max_length = 80
#tokenize te sentences and represent them in numbers other way is to todo one hot 
tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(all_complaint)
sequences = tokenizer_obj.texts_to_sequences(all_complaint)
sequences

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

#get first 10 elements word_index
dict_items = word_index.items()
first_ten = list(dict_items)[:10]
print(first_ten)

#do padding 
lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')
lines_pad

print("Now we have no of complaints as features :",len(lines_pad))
print("Number of target label", len(df['category_id']))

#deine X and y 
X=lines_pad
y=pd.get_dummies(df['category_id']).values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print('Shape of X_train_pad:', X_train.shape)
print('Shape of y_train:', y_train.shape)

print('Shape of X_test_pad:', X_test.shape)
print('Shape of y_test:', y_test.shape)

#LSTM Model creation
embedding_vector_features=100 # i,e number of column of embedding matrix 
# vocab_size NO of words present in the document after one hot 
model= Sequential()
model.add(Embedding(vocab_size,embedding_vector_features,input_length=X.shape[1]))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(16, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history_LSTM = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=30,batch_size=16)

# Plot results
import matplotlib.pyplot as plt

acc = history_LSTM.history['accuracy']
val_acc = history_LSTM.history['val_accuracy']
loss = history_LSTM.history['loss']
val_loss = history_LSTM.history['val_loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs, acc, 'g', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

accr = model.evaluate(X_test,y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

#new complaint 
import numpy as np
new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']
seq = tokenizer_obj.texts_to_sequences(new_complaint)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['Credit reporting, credit repair services, or other personal consumer reports', 'Debt collection', 'Mortgage', 'Credit card or prepaid card', 'Student loan', 'Bank account or service', 'Checking or savings account', 'Consumer Loan', 'Payday loan, title loan, or personal loan', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Money transfers', 'Prepaid card']
print(pred, labels[np.argmax(pred)])

#SAMETHING WE WILL DO WITH WORD 2 VEC PRE TRAINED MODEL